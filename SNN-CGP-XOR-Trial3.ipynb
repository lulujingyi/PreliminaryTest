{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, NamedTuple, Tuple, Any, Sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiking network with the LIF neuron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Spiking function with rectangular gradient.\n",
    "    Source: https://www.frontiersin.org/articles/10.3389/fnins.2018.00331/full\n",
    "    Implementation: https://github.com/combra-lab/pop-spiking-deep-rl/blob/main/popsan_drl/popsan_td3/popsan.py\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, v: torch.Tensor) -> torch.Tensor:\n",
    "        ctx.save_for_backward(v)  # save voltage - thresh for backwards pass\n",
    "        return v.gt(0.0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_output: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        v, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        spike_pseudo_grad = (v.abs() < 0.5).float()  # 0.5 is the width of the rectangle\n",
    "        return grad_input * spike_pseudo_grad, None  # ensure a tuple is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for LIF state\n",
    "class LIFState(NamedTuple):\n",
    "    z: torch.Tensor\n",
    "    v: torch.Tensor\n",
    "    i: torch.Tensor\n",
    "\n",
    "class LIF(nn.Module):\n",
    "    \"\"\"\n",
    "    Leaky-integrate-and-fire neuron with learnable parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        # Initialize all parameters randomly as U(0, 1)\n",
    "        self.i_decay = torch.rand(size) #self.i_decay = nn.Parameter(torch.rand(size))\n",
    "        self.v_decay = torch.rand(size)\n",
    "        self.thresh = torch.rand(size)\n",
    "        self.spike = SpikeFunction.apply  # spike function\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        synapse: nn.Module,\n",
    "        z: torch.Tensor,\n",
    "        state: Optional[LIFState] = None,\n",
    "    ) -> Tuple[torch.Tensor, LIFState]:\n",
    "        # Previous state\n",
    "        if state is None:\n",
    "            state = LIFState(\n",
    "                z=torch.zeros_like(synapse(z)),\n",
    "                v=torch.zeros_like(synapse(z)),\n",
    "                i=torch.zeros_like(synapse(z)),\n",
    "            )\n",
    "        # Update state\n",
    "        i = state.i * self.i_decay + synapse(z)\n",
    "        #print(self.i_decay)\n",
    "        #print(synapse(z))\n",
    "        v = state.v * self.v_decay * (1.0 - state.z) + i\n",
    "        z = self.spike(v - self.thresh)\n",
    "\n",
    "        return z, LIFState(z, v, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Spiking network with LIF neuron model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes: Sequence[int]):\n",
    "        super().__init__()\n",
    "        self.sizes = sizes\n",
    "        self.spike = SpikeFunction.apply\n",
    "\n",
    "        # Define layers\n",
    "        self.synapses = nn.ModuleList()\n",
    "        self.neurons = nn.ModuleList()\n",
    "        self.states = []\n",
    "        # Loop over current (accessible with 'size') and next (accessible with 'sizes[i]') element\n",
    "        for i, size in enumerate(sizes[:-1], start=1):\n",
    "            # Parameters of synapses and neurons are randomly initialized\n",
    "            self.synapses.append(nn.Linear(size, sizes[i], bias=False))\n",
    "            self.neurons.append(LIF(sizes[i]))\n",
    "            self.states.append(None)\n",
    "           \n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        for i, (neuron, synapse) in enumerate(zip(self.neurons, self.synapses)):\n",
    "            z, self.states[i] = neuron(synapse, z, self.states[i])\n",
    "        return z\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resetting states when you're done is very important!\n",
    "        \"\"\"\n",
    "        for i, _ in enumerate(self.states):\n",
    "            self.states[i] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and labels\n",
    "samples = 1000 #10000\n",
    "x = torch.randint(2, (samples, 2)).float()\n",
    "#x = torch.Tensor([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = (x.sum(-1) == 1).float()\n",
    "#print(x)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x,y):\n",
    "        return x+y\n",
    "\n",
    "def sub(x,y):\n",
    "        return x-y\n",
    "\n",
    "def mul(x,y):\n",
    "        return x*y\n",
    "\n",
    "def div(x,y):\n",
    "        return x/y\n",
    "\n",
    "F = [add, sub, mul]#, div]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_operand = 3 #number of operands\n",
    "n_row = 4 #number of rows\n",
    "n_column = 2 #number of columns of internal nodes\n",
    "n_F = len(F) #number of operators\n",
    "\n",
    "def create_node():\n",
    "        internal1 = torch.cat((torch.randint(n_F,(n_row,1)),torch.randint(n_operand,(n_row,n_column))),1)\n",
    "        internal2 = torch.cat((torch.randint(n_F,(n_row,1)),torch.randint(n_row,(n_row,n_column))),1)\n",
    "        internal = torch.cat((internal1,internal2))\n",
    "        return internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(S,operand):\n",
    "        pheno = S[0]\n",
    "        i = S[1]\n",
    "        def compute1(i):\n",
    "                operator = F[pheno[i][0]]\n",
    "                output = operator(operand[pheno[i][1]],operand[pheno[i][2]])\n",
    "                return output\n",
    "        if i < n_row:\n",
    "                output = compute1(i)\n",
    "        else:\n",
    "                operator = F[pheno[i][0]]\n",
    "                output = operator(compute1(pheno[i][1]), compute1(pheno[i][2]))\n",
    "        return output\n",
    "\n",
    "def mutation(S):\n",
    "        mu = torch.randn((8,3))#*0.3\n",
    "        #mu = torch.clamp(mu, -1, 1) + 1\n",
    "        mutated_internal = torch.round(S[0]+mu)\n",
    "        l1 = torch.FloatTensor([[0, 0, 0]])\n",
    "        u1 = torch.FloatTensor([[n_F-1, n_operand-1, n_operand-1]]) #[2,2,2]\n",
    "        mutated_internal[:4] = torch.max(torch.min(mutated_internal[:4], u1), l1) #clamp the nodes in the first column within the range [l1, u1]\n",
    "        l2 = torch.FloatTensor([[0, 0, 0]])\n",
    "        u2 = torch.FloatTensor([[n_F-1, n_row-1, n_row-1]]) #[2,3,3] \n",
    "        mutated_internal[4:] = torch.max(torch.min(mutated_internal[4:], u2), l2) #clamp the nodes in the second column within the range [l2, u2]\n",
    "        mutated_internal = mutated_internal.int()\n",
    "        mutated_node_index = torch.randint(len(mutated_internal),(1,1)).item()\n",
    "        mutated_S = [mutated_internal,mutated_node_index]\n",
    "        return mutated_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(S,snn,trace1,trace2,trace3,loss):\n",
    "    w1 = torch.zeros_like(list(snn.parameters())[0])\n",
    "    for i in range(w1.size()[1]):\n",
    "        for j in range(w1.size()[0]):\n",
    "            w1[j][i] = compute(S,[trace1[i],trace2[j],loss])\n",
    "\n",
    "    w2 = torch.zeros_like(list(snn.parameters())[1])\n",
    "    for i in range(w2.size()[1]):\n",
    "        for j in range(w2.size()[0]):\n",
    "            w2[j][i] = compute(S,[trace2[i],trace3[j],loss])\n",
    "    return w1,w2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = 50\n",
    "epochs = 3\n",
    "decay = 0.5\n",
    "alpha = 1\n",
    "timesteps = 10\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(S,snn):\n",
    "    for e in range(epochs):\n",
    "        losses = []\n",
    "        for i in range(samples):#sample//batch\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            for p in snn.parameters():\n",
    "                p.grad = None\n",
    "            \n",
    "            # Reset the network\n",
    "            snn.reset()\n",
    "\n",
    "            \n",
    "            trace1 = 0\n",
    "            trace2 = 0\n",
    "            trace3 = 0\n",
    "            prediction = torch.Tensor()\n",
    "            \n",
    "            #traces are computed in a timestep equal to the \n",
    "            for d in range(timesteps): \n",
    "                y_hat = snn(x[i]) #i*batch,(i+1)*batch\n",
    "                trace1 = trace1 * decay + alpha * x[i]\n",
    "                trace2 = trace2 * decay + alpha * snn.states[0][0]\n",
    "                trace3 = trace3 * decay + alpha * snn.states[1][0]\n",
    "            prediction = torch.cat((prediction,y_hat))\n",
    "            loss = criterion(prediction, y[i])    \n",
    "            w1,w2 = update_weight(S,snn, trace1, trace2, trace3, (1/(loss.item()+0.01)))\n",
    "            for idx,w in enumerate(snn.parameters()):\n",
    "                    if idx == 0:\n",
    "                        w.data = w1 + w.data\n",
    "                    else:\n",
    "                        w.data = w2 + w.data\n",
    "\n",
    "            # Print statistics\n",
    "            losses.append(loss.item())\n",
    "            #print(f\"[{e + 1}, {i}] loss: {loss.item()}\")\n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 + 4 Evolutionary Strategy: 2 random individuals are intialized randomly and thier losses are computed. For each individual, two offsprings are generated by mutation. If the performane of the offspring is better, the offspring will replace the parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal1 = create_node()\n",
    "\n",
    "node_index1 = torch.randint(len(internal1),(1,1)).item()\n",
    "\n",
    "S1 = [internal1,node_index1]\n",
    "\n",
    "internal2 = create_node()\n",
    "\n",
    "node_index2 = torch.randint(len(internal2),(1,1)).item()\n",
    "\n",
    "S2 = [internal2,node_index2]\n",
    "\n",
    "Stab = [S1,S2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[505.0, 242.0]\n"
     ]
    }
   ],
   "source": [
    "sizes = [2,5,1]\n",
    "L1 = train(S1,SpikingMLP(sizes))\n",
    "L2 = train(S2,SpikingMLP(sizes))\n",
    "Ltab = [L1,L2]\n",
    "print(Ltab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[505.0, 242.0, 242.0, 242.0, 242.0, 505.0]\n",
      "[242.0, 242.0]\n",
      "[242.0, 242.0, 242.0, 505.0, 242.0, 242.0]\n",
      "[242.0, 242.0]\n",
      "[242.0, 242.0, 505.0, 505.0, 242.0, 505.0]\n",
      "[242.0, 242.0]\n",
      "[242.0, 242.0, 505.0, 503.0, 242.0, 242.0]\n",
      "[242.0, 242.0]\n",
      "[0.0, 242.0, 242.0, 471.0, 242.0, 505.0]\n",
      "[0.0, 242.0]\n",
      "[0.0, 242.0, 242.0, 505.0, 505.0, 505.0]\n",
      "[0.0, 242.0]\n",
      "[0.0, 242.0, 242.0, 242.0, 242.0, 518.0]\n",
      "[0.0, 242.0]\n",
      "[0.0, 242.0, 505.0, 242.0, 505.0, 242.0]\n",
      "[0.0, 242.0]\n",
      "[0.0, 229.0, 242.0, 242.0, 242.0, 242.0]\n",
      "[0.0, 229.0]\n",
      "[0.0, 229.0, 242.0, 242.0, 242.0, 242.0]\n",
      "[0.0, 229.0]\n",
      "[0.0, 229.0, 242.0, 242.0, 242.0, 242.0]\n",
      "[0.0, 229.0]\n"
     ]
    }
   ],
   "source": [
    "gen = 101\n",
    "for g in range(gen):\n",
    "    Smutab = []\n",
    "    Smu11 = mutation(Stab[0])\n",
    "    Smu12 = mutation(Stab[0])\n",
    "    Smu21 = mutation(Stab[1])\n",
    "    Smu22 = mutation(Stab[1])\n",
    "    Smutab.append(Smu11)\n",
    "    Smutab.append(Smu12)\n",
    "    Smutab.append(Smu21)\n",
    "    Smutab.append(Smu22)\n",
    "    #print(Smutab)\n",
    "    Lmutab = []\n",
    "    Lmutab.append(train(Smu11,SpikingMLP(sizes)))\n",
    "    Lmutab.append(train(Smu12,SpikingMLP(sizes)))\n",
    "    Lmutab.append(train(Smu21,SpikingMLP(sizes)))\n",
    "    Lmutab.append(train(Smu22,SpikingMLP(sizes)))\n",
    "    #print(Lmutab)\n",
    "    Ljointtab = Ltab + Lmutab\n",
    "    Sjointtab = Stab + Smutab\n",
    "    good_index = heapq.nsmallest(2,range(len(Ljointtab)), Ljointtab.__getitem__)\n",
    "    Stab = [Sjointtab[m] for m in good_index]\n",
    "    Ltab = [Ljointtab[n] for n in good_index]\n",
    "    if g % 10 == 0:\n",
    "        print(Ljointtab)\n",
    "        print(Ltab)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[1, 1, 1],\n",
      "        [1, 2, 1],\n",
      "        [2, 0, 0],\n",
      "        [1, 2, 2],\n",
      "        [0, 2, 0],\n",
      "        [2, 3, 0],\n",
      "        [2, 1, 2],\n",
      "        [2, 3, 0]], dtype=torch.int32), 5], [tensor([[1, 0, 2],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 1],\n",
      "        [0, 1, 0],\n",
      "        [2, 2, 0],\n",
      "        [2, 3, 2],\n",
      "        [1, 3, 0]], dtype=torch.int32), 6]]\n"
     ]
    }
   ],
   "source": [
    "print(Stab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test========================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
